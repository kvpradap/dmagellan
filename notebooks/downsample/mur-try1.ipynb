{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import guppy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from guppy import hpy\n",
    "h = hpy()\n",
    "h.setref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import py_entitymatching as em\n",
    "import cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = em.load_dataset('person_table_A')\n",
    "B = em.load_dataset('person_table_B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess table\n",
    "# 1. concatenate strings, remove punctuations and convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_str_cols(dataframe):\n",
    "    return dataframe.columns[dataframe.dtypes == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "#distutils: language=c++\n",
    "from libcpp.string cimport string\n",
    "from libcpp.vector cimport vector\n",
    "from libcpp.pair cimport pair\n",
    "from libcpp cimport bool\n",
    "from libcpp.set cimport set as oset\n",
    "from libcpp.map cimport map as omap\n",
    "from cython.parallel cimport prange\n",
    "from libcpp.algorithm cimport sort\n",
    "from cython.operator cimport dereference, preincrement\n",
    "import cython\n",
    "import cloudpickle\n",
    "\n",
    "ctypedef cython.int int\n",
    "\n",
    "\n",
    "cdef class StringContainer:\n",
    "\n",
    "    cdef vector[pair[int,string]] container \n",
    "    cdef long size_in_bytes\n",
    "    \n",
    "    cdef void _push_back(self, int i, string s):\n",
    "        self.container.push_back((i, s))\n",
    "    cdef vector[pair[int,string]] _get(self):\n",
    "        return self.container\n",
    "    cdef pair[int, string] _get_index(self, int i) nogil:\n",
    "        return self.container[i]\n",
    "    cdef int _get_size(self) nogil:\n",
    "        return self.container.size()\n",
    "    \n",
    "    cdef long _compute_size(self):\n",
    "        cdef int i\n",
    "        cdef long s = 0        \n",
    "        cdef int_size = 4\n",
    "        if self.size_in_bytes == 0:\n",
    "            for i in range(self._get_size()):\n",
    "                s += len(self.container[i].second)\n",
    "                s += int_size\n",
    "            self.size_in_bytes = s\n",
    "        return self.size_in_bytes\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def push_back(self, int i, string s):\n",
    "        self.container.push_back((i,s))\n",
    "    def get(self):\n",
    "        return self.container\n",
    "    def get_index(self, int i):\n",
    "        return self.container[i]\n",
    "    def get_size(self):\n",
    "        return self._get_size()\n",
    "    \n",
    "    def __sizeof__(self):\n",
    "        return self._compute_size()\n",
    "\n",
    "cdef extern from \"string.h\" nogil:                                                    \n",
    "    #    char *strtok (char *inp_str, const char *delimiters)  \n",
    "        char *strtok_r (char *inp_str, const char *delimiters, char **) \n",
    "# cdef extern from \"<algorithm>\" namespace \"std\" nogil:\n",
    "#         void sort(vector[int].iterator, vector[int].iterator, bool func (pair[int, int], pair[int, int]))\n",
    "        \n",
    "\n",
    "cdef class TokenContainer:\n",
    "    cdef vector[pair[int, vector[string]]] container\n",
    "    cdef long size_in_bytes\n",
    "    \n",
    "    cdef int _get_size(self) nogil:\n",
    "        return self.container.size()\n",
    "\n",
    "    cdef vector[pair[int,vector[string]]] _get(self):\n",
    "        return self.container\n",
    "    \n",
    "    cdef pair[int, vector[string]] _get_index(self, int i) nogil:\n",
    "        return self.container[i]\n",
    "    \n",
    "    cdef long _compute_size(self):\n",
    "        cdef int n = self._get_size()\n",
    "        cdef int i, j\n",
    "        cdef int m\n",
    "        cdef long s = 0\n",
    "        cdef int int_size = 4\n",
    "        cdef vector[string] tokens\n",
    "        print(cython.sizeof(4))\n",
    "        if self.size_in_bytes == 0:\n",
    "            for i in xrange(n):\n",
    "                s += int_size \n",
    "                tokens = self.container[i].second\n",
    "                m = tokens.size()\n",
    "                for j in xrange(m):\n",
    "                    s += len(tokens[j])\n",
    "            self.size_in_bytes = s\n",
    "        return self.size_in_bytes\n",
    "                \n",
    "                \n",
    "    \n",
    "    cdef vector[string] _stokenize(self, const string& inp_string) nogil:\n",
    "        cdef char* ptr1\n",
    "        cdef char* pch = strtok_r(<char*> inp_string.c_str(), \" \\t\\n\", &ptr1)\n",
    "        cdef oset[string] tokens\n",
    "        cdef vector[string] out_tokens\n",
    "        cdef string s\n",
    "        while pch != NULL:\n",
    "            tokens.insert(string(pch))\n",
    "            pch = strtok_r(NULL, \" \\t\\n\", &ptr1)\n",
    "        for s in tokens:\n",
    "            out_tokens.push_back(s)\n",
    "        return out_tokens\n",
    "            \n",
    "    cdef vector[string] _remove_stopwords(self, vector[string] &inp_tokens, const omap[string, int] &stop_words) nogil:\n",
    "        cdef vector[string] out_tokens\n",
    "        cdef string token\n",
    "        for token in inp_tokens:\n",
    "            if (stop_words.find(token) == stop_words.end()):\n",
    "                out_tokens.push_back(token)\n",
    "        return out_tokens\n",
    "    \n",
    "    cdef void _tokenize(self, StringContainer container, omap[string, int] stopwords) nogil:\n",
    "        cdef int n = container._get_size()\n",
    "        cdef int i\n",
    "        cdef string s\n",
    "        cdef int uid\n",
    "        cdef pair[int, string] p\n",
    "        cdef vector[string] out_tokens\n",
    "        for i in xrange(n):\n",
    "            self.container.push_back(pair[int, vector[string]]())\n",
    "        for i in prange(n, nogil=True):\n",
    "            p = container._get_index(i)\n",
    "            uid = p.first\n",
    "            s = p.second\n",
    "            out_tokens = self._stokenize(s)\n",
    "            out_tokens = self._remove_stopwords(out_tokens, stopwords)\n",
    "            self.container[i] = pair[int, vector[string]](uid, out_tokens)\n",
    "            \n",
    "    def get_index(self, int i):\n",
    "        return self._get_index(i)\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self._get_size()\n",
    "                        \n",
    "    def tokenize(self, StringContainer concat_strings, stopwords):\n",
    "        import string as pstring\n",
    "        cdef omap[string, int] stopword_map\n",
    "        str2bytes = lambda x: x if isinstance(x, bytes) else x.encode('utf-8')        \n",
    "        if len(stopwords):\n",
    "            for s in stopwords:                \n",
    "                stopword_map[s] = 0\n",
    "        print('Calling Tokenize')\n",
    "\n",
    "        \n",
    "        self._tokenize(concat_strings, stopword_map)\n",
    "    def __sizeof__(self):\n",
    "        return self._compute_size()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# inverted index\n",
    "cdef class InvertedIndex:\n",
    "    cdef omap[string, vector[int]] index\n",
    "    cdef long size_in_bytes\n",
    "    \n",
    "    cdef vector[int] _get_values(self, string token) nogil:\n",
    "        cdef vector[int] dummy\n",
    "        if self.index.find(token) != self.index.end():\n",
    "            return self.index[token]\n",
    "        else:\n",
    "            return dummy\n",
    "    cdef int _size(self) nogil:\n",
    "        return self.index.size()\n",
    "    \n",
    "#     cdef long _compute_size(self):\n",
    "#         cdef int n = self._size()\n",
    "#         cdef int i\n",
    "#         cdef long s = 0\n",
    "#         cdef omap[string, vector[int]].iterator it = index.begin()\n",
    "#         while it != index.end():\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    cdef void _build_inv_index(self, TokenContainer objtc) nogil:\n",
    "        cdef int n = objtc._get_size()\n",
    "        cdef int m\n",
    "        cdef int i,j\n",
    "        cdef int idx\n",
    "        cdef vector[string] tokens\n",
    "        cdef string token\n",
    "        cdef pair[int, vector[string]] p\n",
    "        for i in xrange(n):\n",
    "            p = objtc._get_index(i)\n",
    "            idx = p.first\n",
    "            tokens = p.second\n",
    "            m = tokens.size()\n",
    "            for j in xrange(m):\n",
    "                self.index[tokens[j]].push_back(idx)\n",
    "    \n",
    "    \n",
    "    def build_inv_index(self, objtc):\n",
    "        self._build_inv_index(objtc)\n",
    "        \n",
    "    def size(self):\n",
    "        return self._size()\n",
    "    \n",
    "    def get_values(self, token):\n",
    "        return self._get_values(token)\n",
    "    def get_inv_index(self):\n",
    "        return self.index\n",
    "        \n",
    "\n",
    "cdef bool comp(const pair[int, int] l, const pair[int, int] r):\n",
    "    return l.second > r.second    \n",
    "\n",
    "# probe\n",
    "cdef class Prober:\n",
    "    cdef vector[pair[int, int]] pair_indices\n",
    "    \n",
    "\n",
    "    \n",
    "    cdef int _size(self):\n",
    "        return self.pair_indices.size()\n",
    "    \n",
    "    cdef pair[int, int] _get_index(self, int i):\n",
    "        return self.pair_indices[i]\n",
    "    \n",
    "    cdef vector[int] _get_ltable_indices(self):\n",
    "        cdef oset[int] set_indices\n",
    "        cdef vector[int] l_indices\n",
    "        cdef int n = self._size()\n",
    "        cdef int i\n",
    "        cdef oset[int].iterator it\n",
    "        for i in xrange(n):\n",
    "            set_indices.insert(self.pair_indices[i].first)\n",
    "        it = set_indices.begin()\n",
    "        while it != set_indices.end():\n",
    "            l_indices.push_back(dereference(it))\n",
    "            preincrement(it)\n",
    "        sort(l_indices.begin(), l_indices.end())\n",
    "        return l_indices\n",
    "\n",
    "    cdef vector[int] _get_rtable_indices(self):\n",
    "        cdef oset[int] set_indices\n",
    "        cdef vector[int] r_indices\n",
    "        cdef int n = self._size()\n",
    "        cdef int i\n",
    "        cdef oset[int].iterator it\n",
    "        for i in xrange(n):\n",
    "            set_indices.insert(self.pair_indices[i].second)\n",
    "        it = set_indices.begin()\n",
    "        while it != set_indices.end():\n",
    "            r_indices.push_back(dereference(it))\n",
    "            preincrement(it)\n",
    "        sort(r_indices.begin(), r_indices.end())\n",
    "        return r_indices\n",
    "        \n",
    "    \n",
    "    cdef void _probe(self, TokenContainer inp_token_list, InvertedIndex index, int y):\n",
    "        cdef int m = inp_token_list._get_size()\n",
    "        cdef int i, j, k, l, r, s\n",
    "        cdef pair[int, vector[string]] p_id_tokens\n",
    "        cdef int uid\n",
    "        cdef int n, o, q\n",
    "        cdef vector[string] tokens\n",
    "        cdef string token\n",
    "        cdef vector[int] candidates\n",
    "        cdef vector[pair[int, int]] to_sort\n",
    "        cdef omap[int, int] candidate_overlap\n",
    "        cdef omap[int,int].iterator it, end\n",
    "        \n",
    "        for i in xrange(m):\n",
    "            p_id_tokens = inp_token_list._get_index(i)\n",
    "            uid = p_id_tokens.first\n",
    "            tokens = p_id_tokens.second\n",
    "            n = tokens.size()\n",
    "            for j in xrange(n):\n",
    "                token = tokens[j]\n",
    "#                 print(token)\n",
    "                candidates = index._get_values(token)\n",
    "                o = candidates.size()\n",
    "                for k in xrange(o):\n",
    "                    candidate_overlap[candidates[k]] += 1\n",
    "            it = candidate_overlap.begin()\n",
    "            end = candidate_overlap.end()\n",
    "            while it != end:\n",
    "                to_sort.push_back(dereference(it))\n",
    "                preincrement(it)\n",
    "            sort(to_sort.begin(), to_sort.end(), comp)\n",
    "            q = 0\n",
    "            # print \n",
    "#             for k in xrange(to_sort.size()):\n",
    "#                 print('{0}: {1}'.format(to_sort[k].first, to_sort[k].second))\n",
    "            for k in xrange(to_sort.size()):\n",
    "                if q == y:\n",
    "                    break\n",
    "                \n",
    "                self.pair_indices.push_back(pair[int, int](to_sort[k].first, uid))\n",
    "                q += 1\n",
    "            candidate_overlap.clear()\n",
    "            to_sort.clear()\n",
    "\n",
    "    def probe(self, TokenContainer inp_token_list, InvertedIndex index, int y):\n",
    "        self._probe(inp_token_list, index, y)\n",
    "    def get_ltable_indices(self):\n",
    "        return self._get_ltable_indices()\n",
    "    def get_rtable_indices(self):\n",
    "        return self._get_rtable_indices()\n",
    "    def get_all(self):\n",
    "        return self.pair_indices\n",
    "    def get_index(self, i):\n",
    "        return self._get_index(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def preprocess_table(dataframe):\n",
    "    str_cols = get_str_cols(dataframe)\n",
    "    projected_df = dataframe[str_cols]\n",
    "    concat_strings = []\n",
    "    str2bytes = lambda x: x if isinstance(x, bytes) else x.encode('utf-8')\n",
    "    str_container_obj = StringContainer()\n",
    "    for row in projected_df.itertuples(name=None):\n",
    "        idx = row[0]\n",
    "        joined_row = ' '.join(row[1:])\n",
    "        joined_row = joined_row.translate(None, string.punctuation)\n",
    "        concat_strings.append(joined_row.lower())\n",
    "        str_container_obj.push_back(idx, str2bytes(joined_row.lower()))\n",
    "        \n",
    "    return str_container_obj\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_strings(concat_strings, stopwords):\n",
    "    n = concat_strings.get_size()\n",
    "    tok_container_obj = TokenContainer()\n",
    "    tok_container_obj.tokenize(concat_strings, stopwords)\n",
    "    return tok_container_obj\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inv_index(tokens):\n",
    "    inv_obj = InvertedIndex()\n",
    "    inv_obj.build_inv_index(tokens)\n",
    "    return inv_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probe(tokens, invindex, y):\n",
    "    probe_obj = Prober()\n",
    "    probe_obj.probe(tokens, invindex, y)\n",
    "    return probe_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Tokenize\n",
      "Calling Tokenize\n"
     ]
    }
   ],
   "source": [
    "lconcat_strings = preprocess_table(A)\n",
    "rconcat_strings = preprocess_table(B)\n",
    "stopwords=['san', 'st', 'a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4']\n",
    "ltokens = tokenize_strings(lconcat_strings, stopwords)\n",
    "rtokens = tokenize_strings(rconcat_strings, stopwords)\n",
    "y = 1\n",
    "inv_index = build_inv_index(ltokens)\n",
    "probe_result = probe(rtokens, inv_index, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (2, 1), (1, 2), (0, 3), (4, 4), (1, 5)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe_result.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ltable_indices = probe_result.get_ltable_indices()\n",
    "rtable_indices = probe_result.get_rtable_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5], [0, 1, 2, 4])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtable_indices, ltable_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(rtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(cython.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>hourly_wage</th>\n",
       "      <th>address</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>Kevin Smith</td>\n",
       "      <td>1989</td>\n",
       "      <td>30.0</td>\n",
       "      <td>607 From St, San Francisco</td>\n",
       "      <td>94107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>Michael Franklin</td>\n",
       "      <td>1988</td>\n",
       "      <td>27.5</td>\n",
       "      <td>1652 Stockton St, San Francisco</td>\n",
       "      <td>94122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>William Bridge</td>\n",
       "      <td>1986</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3131 Webster St, San Francisco</td>\n",
       "      <td>94107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4</td>\n",
       "      <td>Binto George</td>\n",
       "      <td>1987</td>\n",
       "      <td>32.5</td>\n",
       "      <td>423 Powell St, San Francisco</td>\n",
       "      <td>94122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a5</td>\n",
       "      <td>Alphonse Kemper</td>\n",
       "      <td>1984</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1702 Post Street, San Francisco</td>\n",
       "      <td>94122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID              name  birth_year  hourly_wage  \\\n",
       "0  a1       Kevin Smith        1989         30.0   \n",
       "1  a2  Michael Franklin        1988         27.5   \n",
       "2  a3    William Bridge        1986         32.0   \n",
       "3  a4      Binto George        1987         32.5   \n",
       "4  a5   Alphonse Kemper        1984         35.0   \n",
       "\n",
       "                           address  zipcode  \n",
       "0       607 From St, San Francisco    94107  \n",
       "1  1652 Stockton St, San Francisco    94122  \n",
       "2   3131 Webster St, San Francisco    94107  \n",
       "3     423 Powell St, San Francisco    94122  \n",
       "4  1702 Post Street, San Francisco    94122  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'a1 kevin smith 607 from st san francisco')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs.get_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Tokenize\n"
     ]
    }
   ],
   "source": [
    "objtc = tokenize_strings(objs, ['san'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = inv_index(objtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.get_values('francisco')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
