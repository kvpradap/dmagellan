{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import py_entitymatching as em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = em.load_dataset('person_table_A')\n",
    "B = em.load_dataset('person_table_B')\n",
    "\n",
    "def get_str_cols(dataframe):\n",
    "    return dataframe.columns[dataframe.dtypes == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def _get_stop_words():\n",
    "    stop_words_set = set()\n",
    "    install_path = em.get_install_path()\n",
    "    dataset_path = os.sep.join([install_path, 'utils'])\n",
    "    stop_words_file = os.sep.join([dataset_path, 'stop_words.txt'])\n",
    "    with open(stop_words_file, \"rb\") as stopwords_file:\n",
    "        for stop_words in stopwords_file:\n",
    "            stop_words_set.add(stop_words.rstrip())\n",
    "\n",
    "    return stop_words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'six',\n",
       " 'less',\n",
       " 'being',\n",
       " 'indeed',\n",
       " 'over',\n",
       " 'move',\n",
       " 'anyway',\n",
       " 'four',\n",
       " 'not',\n",
       " 'own',\n",
       " 'through',\n",
       " 'yourselves',\n",
       " 'fify',\n",
       " 'where',\n",
       " 'mill',\n",
       " 'only',\n",
       " 'find',\n",
       " 'before',\n",
       " 'one',\n",
       " 'whose',\n",
       " 'system',\n",
       " 'how',\n",
       " 'somewhere',\n",
       " 'with',\n",
       " 'show',\n",
       " 'had',\n",
       " 'enough',\n",
       " 'should',\n",
       " 'to',\n",
       " 'must',\n",
       " 'whom',\n",
       " 'seeming',\n",
       " 'whole',\n",
       " 'under',\n",
       " 'ours',\n",
       " 'has',\n",
       " 'might',\n",
       " 'thereafter',\n",
       " 'latterly',\n",
       " 'do',\n",
       " 'them',\n",
       " 'his',\n",
       " 'around',\n",
       " 'than',\n",
       " 'get',\n",
       " 'very',\n",
       " 'de',\n",
       " 'none',\n",
       " 'cannot',\n",
       " 'every',\n",
       " 'whether',\n",
       " 'they',\n",
       " 'front',\n",
       " 'during',\n",
       " 'thus',\n",
       " 'now',\n",
       " 'him',\n",
       " 'nor',\n",
       " 'name',\n",
       " 'several',\n",
       " 'hereafter',\n",
       " 'always',\n",
       " 'who',\n",
       " 'cry',\n",
       " 'whither',\n",
       " 'this',\n",
       " 'someone',\n",
       " 'either',\n",
       " 'each',\n",
       " 'become',\n",
       " 'thereupon',\n",
       " 'sometime',\n",
       " 'side',\n",
       " 'two',\n",
       " 'therein',\n",
       " 'twelve',\n",
       " 'because',\n",
       " 'often',\n",
       " 'ten',\n",
       " 'our',\n",
       " 'eg',\n",
       " 'some',\n",
       " 'back',\n",
       " 'thickv',\n",
       " 'go',\n",
       " 'namely',\n",
       " 'towards',\n",
       " 'are',\n",
       " 'further',\n",
       " 'beyond',\n",
       " 'ourselves',\n",
       " 'yet',\n",
       " 'out',\n",
       " 'even',\n",
       " 'will',\n",
       " 'what',\n",
       " 'still',\n",
       " 'for',\n",
       " 'bottom',\n",
       " 'mine',\n",
       " 'since',\n",
       " 'please',\n",
       " 'forty',\n",
       " 'per',\n",
       " 'its',\n",
       " 'everything',\n",
       " 'behind',\n",
       " 'un',\n",
       " 'above',\n",
       " 'between',\n",
       " 'it',\n",
       " 'neither',\n",
       " 'seemed',\n",
       " 'ever',\n",
       " 'across',\n",
       " 'she',\n",
       " 'somehow',\n",
       " 'be',\n",
       " 'we',\n",
       " 'full',\n",
       " 'never',\n",
       " 'sixty',\n",
       " 'however',\n",
       " 'here',\n",
       " 'otherwise',\n",
       " 'were',\n",
       " 'whereupon',\n",
       " 'nowhere',\n",
       " 'although',\n",
       " 'found',\n",
       " 'alone',\n",
       " 're',\n",
       " 'along',\n",
       " 'fifteen',\n",
       " 'by',\n",
       " 'both',\n",
       " 'about',\n",
       " 'last',\n",
       " 'would',\n",
       " 'anything',\n",
       " 'via',\n",
       " 'many',\n",
       " 'could',\n",
       " 'thence',\n",
       " 'put',\n",
       " 'against',\n",
       " 'keep',\n",
       " 'etc',\n",
       " 'amount',\n",
       " 'became',\n",
       " 'ltd',\n",
       " 'hence',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'con',\n",
       " 'among',\n",
       " 'already',\n",
       " 'co',\n",
       " 'afterwards',\n",
       " 'formerly',\n",
       " 'within',\n",
       " 'seems',\n",
       " 'into',\n",
       " 'others',\n",
       " 'while',\n",
       " 'whatever',\n",
       " 'except',\n",
       " 'down',\n",
       " 'hers',\n",
       " 'everyone',\n",
       " 'done',\n",
       " 'least',\n",
       " 'another',\n",
       " 'whoever',\n",
       " 'moreover',\n",
       " 'couldnt',\n",
       " 'throughout',\n",
       " 'anyhow',\n",
       " 'yourself',\n",
       " 'three',\n",
       " 'from',\n",
       " 'her',\n",
       " 'few',\n",
       " 'together',\n",
       " 'top',\n",
       " 'there',\n",
       " 'due',\n",
       " 'been',\n",
       " 'next',\n",
       " 'anyone',\n",
       " 'eleven',\n",
       " 'much',\n",
       " 'call',\n",
       " 'therefore',\n",
       " 'interest',\n",
       " 'then',\n",
       " 'thru',\n",
       " 'themselves',\n",
       " 'hundred',\n",
       " 'was',\n",
       " 'sincere',\n",
       " 'empty',\n",
       " 'more',\n",
       " 'himself',\n",
       " 'elsewhere',\n",
       " 'mostly',\n",
       " 'on',\n",
       " 'fire',\n",
       " 'am',\n",
       " 'becoming',\n",
       " 'hereby',\n",
       " 'amongst',\n",
       " 'else',\n",
       " 'part',\n",
       " 'everywhere',\n",
       " 'too',\n",
       " 'herself',\n",
       " 'former',\n",
       " 'those',\n",
       " 'he',\n",
       " 'me',\n",
       " 'myself',\n",
       " 'made',\n",
       " 'twenty',\n",
       " 'these',\n",
       " 'bill',\n",
       " 'cant',\n",
       " 'us',\n",
       " 'until',\n",
       " 'besides',\n",
       " 'whenever',\n",
       " 'below',\n",
       " 'anywhere',\n",
       " 'nine',\n",
       " 'can',\n",
       " 'of',\n",
       " 'your',\n",
       " 'toward',\n",
       " 'my',\n",
       " 'something',\n",
       " 'and',\n",
       " 'whereafter',\n",
       " 'give',\n",
       " 'almost',\n",
       " 'wherever',\n",
       " 'is',\n",
       " 'describe',\n",
       " 'beforehand',\n",
       " 'herein',\n",
       " 'an',\n",
       " 'as',\n",
       " 'itself',\n",
       " 'at',\n",
       " 'have',\n",
       " 'in',\n",
       " 'seem',\n",
       " 'whence',\n",
       " 'ie',\n",
       " 'any',\n",
       " 'fill',\n",
       " 'again',\n",
       " 'hasnt',\n",
       " 'inc',\n",
       " 'thereby',\n",
       " 'thin',\n",
       " 'no',\n",
       " 'perhaps',\n",
       " 'latter',\n",
       " 'meanwhile',\n",
       " 'when',\n",
       " 'detail',\n",
       " 'same',\n",
       " 'wherein',\n",
       " 'beside',\n",
       " 'also',\n",
       " 'that',\n",
       " 'other',\n",
       " 'take',\n",
       " 'which',\n",
       " 'becomes',\n",
       " 'you',\n",
       " 'if',\n",
       " 'nobody',\n",
       " 'see',\n",
       " 'though',\n",
       " 'may',\n",
       " 'after',\n",
       " 'upon',\n",
       " 'most',\n",
       " 'hereupon',\n",
       " 'eight',\n",
       " 'but',\n",
       " 'serious',\n",
       " 'nothing',\n",
       " 'such',\n",
       " 'why',\n",
       " 'a',\n",
       " 'off',\n",
       " 'whereby',\n",
       " 'third',\n",
       " 'nevertheless',\n",
       " 'up',\n",
       " 'noone',\n",
       " 'sometimes',\n",
       " 'well',\n",
       " 'amoungst',\n",
       " 'yours',\n",
       " 'their',\n",
       " 'rather',\n",
       " 'without',\n",
       " 'so',\n",
       " 'five',\n",
       " 'the',\n",
       " 'first',\n",
       " 'whereas',\n",
       " 'once',\n",
       " 'the',\n",
       " 'my',\n",
       " 'i',\n",
       " 'andre',\n",
       " 'from']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "#distutils: language=c++\n",
    "from libcpp.vector cimport vector\n",
    "from libcpp.string cimport string\n",
    "from libcpp.map cimport map as omap\n",
    "from libcpp.set cimport set as oset\n",
    "from libcpp.algorithm cimport sort\n",
    "from libcpp cimport bool\n",
    "from libcpp.pair cimport pair\n",
    "cdef extern from \"string.h\":\n",
    "    char *strtok_r (char *inp_str, const char *delimiters, char **)\n",
    "########################################\n",
    "cdef class StringContainer:\n",
    "    cdef vector[string] sc\n",
    "    cdef int csize(self):\n",
    "        return self.sc.size()\n",
    "\n",
    "        \n",
    "    \n",
    "    def size(self):\n",
    "        return self.sc.size()\n",
    "    def push_back(self, s):\n",
    "        self.sc.push_back(s)\n",
    "    def get(self, int i):\n",
    "        return self.sc[i]\n",
    "########################################\n",
    "cdef class TokenContainer:\n",
    "    cdef vector[vector[string]] tc\n",
    "    \n",
    "    cdef int csize(self):\n",
    "        return self.tc.size()\n",
    "    \n",
    "    cdef void cinit(self, n):\n",
    "        cdef int i\n",
    "        for i in xrange(n):\n",
    "            self.tc.push_back(vector[string]())\n",
    "    cdef void cpush_back(self, vector[string] tokens):\n",
    "        self.tc.push_back(tokens)\n",
    "\n",
    "    cdef vector[string] cremove_stopwords(self,  vector[string]& svec, \\\n",
    "                                           omap[string, int]& stopwords):\n",
    "        cdef vector[string] ovec\n",
    "        cdef string token\n",
    "        for token in svec:\n",
    "            if (stopwords.find(token) == stopwords.end()):\n",
    "                ovec.push_back(token)\n",
    "        return ovec\n",
    "        \n",
    "    cdef vector[string] ctokenize_wd(self, const string& inp):\n",
    "        cdef char* ptr1\n",
    "        cdef char* pch = strtok_r(<char*> inp.c_str(), \" \\t\\n\", &ptr1)\n",
    "        cdef oset[string] tokens\n",
    "        cdef vector[string] out\n",
    "        cdef string s\n",
    "        while pch != NULL:\n",
    "            tokens.insert(string(pch))\n",
    "            pch = strtok_r(NULL, \" \\t\\n\", &ptr1)\n",
    "        for s in tokens:\n",
    "            out.push_back(s)\n",
    "        return out\n",
    "        \n",
    "    cdef void ctokenize(self, vector[string]& svec,  omap[string, int]& stopwords):\n",
    "        cdef int n = svec.size()\n",
    "        cdef int i\n",
    "        cdef string s\n",
    "        cdef vector[string] tokens\n",
    "        \n",
    "        self.cinit(n)\n",
    "        \n",
    "        for i in xrange(n):\n",
    "            s = svec[i]\n",
    "            tokens = self.ctokenize_wd(s)\n",
    "            tokens = self.cremove_stopwords(tokens, stopwords)\n",
    "            self.tc[i] = tokens\n",
    "        \n",
    "    def tokenize(self, StringContainer objsc, stopwords):\n",
    "        cdef omap[string, int] smap\n",
    "        str2bytes = lambda x: x if isinstance(x, bytes) else x.encode('utf-8')\n",
    "        if len(stopwords):\n",
    "            for s in stopwords:\n",
    "                smap[s] = 0\n",
    "        self.ctokenize(objsc.sc, smap)\n",
    "    \n",
    "    def get(self, int i):\n",
    "        return self.tc[i]\n",
    "    \n",
    "    def size(self):\n",
    "        return self.csize()\n",
    " ########################################   \n",
    "\n",
    "cdef class InvertedIndex:\n",
    "    cdef omap[string, vector[int]] index\n",
    "    \n",
    "    cdef vector[int] cvalues(self, string token):\n",
    "        cdef vector[int] tmp\n",
    "        if self.index.find(token) != self.index.end():\n",
    "            return self.index[token]\n",
    "        else:\n",
    "            return tmp\n",
    "    \n",
    "    cdef void cbuild_inv_index(self, vector[vector[string]]& token_vector):\n",
    "        cdef int n = token_vector.size()\n",
    "        cdef int i, j\n",
    "        cdef int m\n",
    "        cdef vector[string] tokens\n",
    "        \n",
    "        for i in xrange(n):\n",
    "            tokens = token_vector[i]\n",
    "            m = tokens.size()\n",
    "            for j in xrange(m):\n",
    "                self.index[tokens[j]].push_back(i)\n",
    "            \n",
    "    def build_inv_index(self, TokenContainer objtc):\n",
    "        self.cbuild_inv_index(objtc.tc)\n",
    "        \n",
    "    def values(self, token):\n",
    "        return self.cvalues(token)\n",
    "    \n",
    "    \n",
    " ########################################  \n",
    "cdef bool comp(const pair[int, int] l, const pair[int, int] r):\n",
    "    return l.second > r.second \n",
    "cdef class Prober:\n",
    "    cdef vector[int] llocs\n",
    "    cdef vector[int] rlocs\n",
    "    \n",
    "    cdef int clsize(self):\n",
    "        return self.llocs.size()\n",
    "    \n",
    "    cdef int crsize(self):\n",
    "        return self.rlocs.size()\n",
    "    \n",
    "    cdef vector[int] cget_llocs(self):\n",
    "        sort(self.llocs.begin(), self.llocs.end())\n",
    "        return self.llocs\n",
    "    cdef vector[int] cget_rlocs(self):\n",
    "        return self.rlocs\n",
    "        \n",
    "\n",
    "    cdef inline vector[int] cvalues(self, omap[string, vector[int]]& index, string token):\n",
    "        cdef vector[int] tmp\n",
    "        if index.find(token) != index.end():\n",
    "            return index[token]\n",
    "        else:\n",
    "            return tmp\n",
    "    \n",
    "    \n",
    "    cdef void cprobe(self, vector[vector[string]]& token_vector, \\\n",
    "                     vector[int] ids,\n",
    "                     omap[string, vector[int]]& index, int yparam):\n",
    "        cdef int m, n\n",
    "        cdef int i, j, k\n",
    "        cdef vector[string] tokens\n",
    "        cdef oset[int] lset, rset\n",
    "        cdef vector[int] candidates\n",
    "        cdef omap[int, int] cand_overlap\n",
    "        cdef pair[int, int] entry\n",
    "        cdef vector[pair[int, int]] tmp\n",
    "        cdef int rid\n",
    "        cdef int mx = 0\n",
    "        n = token_vector.size()\n",
    "        for i in xrange(n):\n",
    "            tokens = token_vector[i]\n",
    "            rid = ids[i]\n",
    "            for j in xrange(tokens.size()):\n",
    "                candidates = self.cvalues(index, tokens[j])\n",
    "#                 print(tokens[j])\n",
    "#                 print(candidates)\n",
    "#                 if candidates.size() > mx:\n",
    "#                     mx = candidates.size()\n",
    "#                     print(tokens[j])\n",
    "#                     print(mx)\n",
    "                for cand in candidates:\n",
    "                    cand_overlap[cand] += 1\n",
    "            if cand_overlap.size():\n",
    "                rset.insert(i)\n",
    "            for entry in cand_overlap:\n",
    "                tmp.push_back(entry)\n",
    "            sort(tmp.begin(), tmp.end(), comp)\n",
    "            k = 0\n",
    "            for entry in tmp:\n",
    "                lset.insert(entry.first)\n",
    "                k += 1\n",
    "                if k == yparam:\n",
    "                    break\n",
    "            cand_overlap.clear()\n",
    "            tmp.clear()\n",
    "        for i in lset:\n",
    "            self.llocs.push_back(i)\n",
    "        for i in rset:\n",
    "            self.rlocs.push_back(i)\n",
    "    \n",
    "    def probe(self, TokenContainer objtc, ids, InvertedIndex index, yparam):\n",
    "        self.cprobe(objtc.tc, ids, index.index, yparam)\n",
    "    \n",
    "    def get_lids(self):\n",
    "        return self.cget_llocs()\n",
    "    def get_rids(self):\n",
    "        return self.cget_rlocs()\n",
    "        \n",
    "                \n",
    "                \n",
    "                    \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "# def preprocess_table(dataframe):\n",
    "#     str_cols = get_str_cols(dataframe)\n",
    "#     projected_df = dataframe[str_cols]\n",
    "#     concat_strings = []\n",
    "\n",
    "#     str_container_obj = StringContainer()\n",
    "#     for row in projected_df.itertuples(name=None):\n",
    "#         idx = row[0]\n",
    "#         joined_row = ' '.join(row[1:])\n",
    "#         joined_row = joined_row.translate(None, string.punctuation)\n",
    "#         concat_strings.append(joined_row.lower())\n",
    "#         str_container_obj.push_back(str2bytes(joined_row.lower()))\n",
    "        \n",
    "#     return str_container_obj\n",
    "\n",
    "def preprocess_table(dataframe):\n",
    "    str_cols = get_str_cols(dataframe)\n",
    "    proj_df = dataframe[str_cols]\n",
    "    concat_strings = []\n",
    "    str_container = StringContainer()\n",
    "    str2bytes = lambda x: x if isinstance(x, bytes) else x.encode('utf-8')\n",
    "    for row in proj_df.itertuples(name=None):\n",
    "        idx = row[0]\n",
    "        column_values = row[1:]\n",
    "        strs = [column_value.strip() for column_value in column_values if not pd.isnull(column_value)]\n",
    "        joined_row = ' '.join(strs)\n",
    "        joined_row = joined_row.translate(None, string.punctuation)\n",
    "        concat_strings.append(joined_row.lower())\n",
    "        str_container.push_back(str2bytes(joined_row.lower()))\n",
    "    return str_container\n",
    "\n",
    "\n",
    "def tokenize_strings(concat_strings, stopwords):\n",
    "    n = concat_strings.size()\n",
    "    tok_container_obj = TokenContainer()\n",
    "    tok_container_obj.tokenize(concat_strings, stopwords)\n",
    "    return tok_container_obj\n",
    "\n",
    "def build_inv_index(tokens):\n",
    "    inv_obj = InvertedIndex()\n",
    "    inv_obj.build_inv_index(tokens)\n",
    "    return inv_obj\n",
    "\n",
    "def probe(tokens, n, invindex, y):\n",
    "    probe_obj = Prober()\n",
    "    probe_obj.probe(tokens, range(n), invindex, y)\n",
    "    return probe_obj\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "C = pd.read_csv('songs.csv')\n",
    "D = pd.read_csv('tracks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(961593, 734485)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(C), len(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = list(_get_stop_words())\n",
    "# stopwords.extend(['the', 'my', 'i', 'andre', 'from', 'a', 'of', 'the', 'version', 'love', 'live', 'la', 'mix', 'album', \\\n",
    "#                   'dont', 'remix', 'feat'])\n",
    "stopwords.extend(['the', 'my', 'i', 'andre', 'from', 'a', 'of', 'the', 'version', 'love', 'live', 'la', 'mix', 'album', \\\n",
    "                  'dont'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.24 s, sys: 299 ms, total: 4.53 s\n",
      "Wall time: 4.58 s\n",
      "CPU times: user 46.2 ms, sys: 20.2 ms, total: 66.4 ms\n",
      "Wall time: 66.8 ms\n",
      "CPU times: user 72 ms, sys: 4.12 ms, total: 76.2 ms\n",
      "Wall time: 73.7 ms\n",
      "CPU times: user 5.87 s, sys: 110 ms, total: 5.98 s\n",
      "Wall time: 6.01 s\n",
      "CPU times: user 122 ms, sys: 878 Âµs, total: 123 ms\n",
      "Wall time: 124 ms\n",
      "CPU times: user 6.02 s, sys: 121 ms, total: 6.14 s\n",
      "Wall time: 6.34 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%time lconcat_strings = preprocess_table(C)\n",
    "%time D1 = D.sample(10000, replace=False)\n",
    "%time rconcat_strings = preprocess_table(D1)\n",
    "# %time stopwords=['san', 'st', 'francisco']\n",
    "%time ltokens = tokenize_strings(lconcat_strings, stopwords)\n",
    "%time rtokens = tokenize_strings(rconcat_strings, stopwords)\n",
    "%time inv_index = build_inv_index(ltokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import py_entitymatching as em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.8 s, sys: 658 ms, total: 42.4 s\n",
      "Wall time: 45.7 s\n"
     ]
    }
   ],
   "source": [
    "%time probe_res = probe(rtokens, rtokens.size(), inv_index, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rids = (probe_res.get_rids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>episode</th>\n",
       "      <th>song</th>\n",
       "      <th>artists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222149</th>\n",
       "      <td>222149</td>\n",
       "      <td>SpongeBob SquarePants</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Jellyfishing/Plankton! (#1.3)</td>\n",
       "      <td>House of Horror</td>\n",
       "      <td>w. merrick farran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245180</th>\n",
       "      <td>245180</td>\n",
       "      <td>The Fosters</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Vigil (#1.9)</td>\n",
       "      <td>On the Other Side</td>\n",
       "      <td>phillip larue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325999</th>\n",
       "      <td>325999</td>\n",
       "      <td>Til There Was You</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One Big Happy Family</td>\n",
       "      <td>winnie holzman+david evans+brock walsh+the walsh family singers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619867</th>\n",
       "      <td>619867</td>\n",
       "      <td>Smothered</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Complete</td>\n",
       "      <td>kathryn sutherland+push3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357595</th>\n",
       "      <td>357595</td>\n",
       "      <td>Backroads to Vegas</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love Notes Intro</td>\n",
       "      <td>kirsten vogelsang+tony cultreri+mona gable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                  title    year                        episode  \\\n",
       "222149  222149  SpongeBob SquarePants  1999.0  Jellyfishing/Plankton! (#1.3)   \n",
       "245180  245180            The Fosters  2013.0                   Vigil (#1.9)   \n",
       "325999  325999      Til There Was You  1997.0                            NaN   \n",
       "619867  619867              Smothered  2002.0                            NaN   \n",
       "357595  357595     Backroads to Vegas  1996.0                            NaN   \n",
       "\n",
       "                        song  \\\n",
       "222149       House of Horror   \n",
       "245180     On the Other Side   \n",
       "325999  One Big Happy Family   \n",
       "619867              Complete   \n",
       "357595      Love Notes Intro   \n",
       "\n",
       "                                                                artists  \n",
       "222149                                                w. merrick farran  \n",
       "245180                                                    phillip larue  \n",
       "325999  winnie holzman+david evans+brock walsh+the walsh family singers  \n",
       "619867                                         kathryn sutherland+push3  \n",
       "357595                       kirsten vogelsang+tony cultreri+mona gable  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1.iloc[rids].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lconcat_strings.get(0)\n",
    "ltokens.get(0)\n",
    "inv_index.values('francisco')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
